{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25cb7590",
   "metadata": {},
   "source": [
    "This page first shows how to visualize higher dimension data using various Plotly figures combined with dimensionality reduction (aka projection). Then, we dive into the specific details of our projection algorithm.\n",
    "\n",
    "We will use [Scikit-learn](https://scikit-learn.org/) to load one of the datasets, and apply dimensionality reduction. Scikit-learn is a popular Machine Learning (ML) library that offers various tools for creating and training ML algorithms, feature engineering, data cleaning, and evaluating and testing models. It was designed to be accessible, and to work seamlessly with popular libraries like NumPy and Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82452866",
   "metadata": {},
   "source": [
    "## High-dimensional PCA Analysis with  `px.scatter_matrix`\n",
    "\n",
    "The dimensionality reduction technique we will be using is called the [Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/decomposition.html#pca). It is a powerful technique that arises from linear algebra and probability theory. In essence, it computes a matrix that represents the variation of your data ([covariance matrix/eigenvectors][covmatrix]), and rank them by their relevance (explained variance/eigenvalues). For a video tutorial, see [this segment on PCA](https://youtu.be/rng04VJxUt4?t=98) from the Coursera ML course.\n",
    "\n",
    "[covmatrix]: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues#:~:text=As%20it%20is%20a%20square%20symmetric%20matrix%2C%20it%20can%20be%20diagonalized%20by%20choosing%20a%20new%20orthogonal%20coordinate%20system%2C%20given%20by%20its%20eigenvectors%20(incidentally%2C%20this%20is%20called%20spectral%20theorem)%3B%20corresponding%20eigenvalues%20will%20then%20be%20located%20on%20the%20diagonal.%20In%20this%20new%20coordinate%20system%2C%20the%20covariance%20matrix%20is%20diagonal%20and%20looks%20like%20that%3A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb9d90",
   "metadata": {},
   "source": [
    "### Visualize all the original dimensions\n",
    "\n",
    "First, let's plot all the features and see how the `species` in the Iris dataset are grouped. In a [Scatter Plot Matrix (splom)](https://plot.ly/python/splom/), each subplot displays a feature against another, so if we have $N$ features we have a $N \\times N$ matrix.\n",
    "\n",
    "In our example, we are plotting all 4 features from the Iris dataset, thus we can see how `sepal_width` is compared against `sepal_length`, then against `petal_width`, and so forth. Keep in mind how some pairs of features can more easily separate different species.\n",
    "\n",
    "In this example, we will use [Plotly Express](/python/plotly-express/), Plotly's high-level API for building figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ea264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df = px.data.iris()\n",
    "features = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    df,\n",
    "    dimensions=features,\n",
    "    color=\"species\"\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639de77",
   "metadata": {},
   "source": [
    "###  Visualize all the principal components\n",
    "\n",
    "Now, we apply `PCA` the same dataset, and retrieve **all** the components. We use the same `px.scatter_matrix` trace to display our results, but this time our features are the resulting *principal components*, ordered by how much variance they are able to explain.\n",
    "\n",
    "The importance of explained variance is demonstrated in the example below. The subplot between PC3 and PC4 is clearly unable to separate each class, whereas the subplot between PC1 and PC2 shows a clear separation between each species.\n",
    "\n",
    "In this example, we will use [Plotly Express](/python/plotly-express/), Plotly's high-level API for building figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd53c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = px.data.iris()\n",
    "features = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]\n",
    "\n",
    "pca = PCA()\n",
    "components = pca.fit_transform(df[features])\n",
    "labels = {\n",
    "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    labels=labels,\n",
    "    dimensions=range(4),\n",
    "    color=df[\"species\"]\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd02555",
   "metadata": {},
   "source": [
    "### Visualize a subset of the principal components\n",
    "\n",
    "When you will have too many features to visualize, you might be interested in only visualizing the most relevant components. Those components often capture a majority of the [explained variance](https://en.wikipedia.org/wiki/Explained_variation), which is a good way to tell if those components are sufficient for modelling this dataset.\n",
    "\n",
    "In the example below, our dataset contains 8 features, but we only select the first 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.data\n",
    "n_components = 2\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "components = pca.fit_transform(df)\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "labels = {str(i): f\"PC {i+1}\" for i in range(n_components)}\n",
    "labels['color'] = 'Median Price'\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    color=housing.target,\n",
    "    dimensions=range(n_components),\n",
    "    labels=labels,\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54332de7",
   "metadata": {},
   "source": [
    "## PCA analysis in Dash\n",
    "\n",
    "[Dash](https://plotly.com/dash/) is the best way to build analytical apps in Python using Plotly figures. To run the app below, run `pip install dash`, click \"Download\" to get the code and run `python app.py`.\n",
    "\n",
    "Get started  with [the official Dash docs](https://dash.plotly.com/installation) and **learn how to effortlessly [style](https://plotly.com/dash/design-kit/) & [deploy](https://plotly.com/dash/app-manager/) apps like this with <a class=\"plotly-red\" href=\"https://plotly.com/dash/\">Dash Enterprise</a>.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb8e36",
   "metadata": {
    "hide_code": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "snippet_url = 'https://python-docs-dash-snippets.herokuapp.com/python-docs-dash-snippets/'\n",
    "IFrame(snippet_url + 'pca-visualization', width='100%', height=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf40b58",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 0.9em;\"><div style=\"width: calc(100% - 30px); box-shadow: none; border: thin solid rgb(229, 229, 229);\"><div style=\"padding: 5px;\"><div><p><strong>Sign up for Dash Club</strong> â†’ Free cheat sheets plus updates from Chris Parmer and Adam Schroeder delivered to your inbox every two months. Includes tips and tricks, community apps, and deep dives into the Dash architecture.\n",
    "<u><a href=\"https://go.plotly.com/dash-club?utm_source=Dash+Club+2022&utm_medium=graphing_libraries&utm_content=inline\">Join now</a></u>.</p></div></div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719fa29",
   "metadata": {},
   "source": [
    "## 2D PCA Scatter Plot\n",
    "\n",
    "In the previous examples, you saw how to visualize high-dimensional PCs. In this example, we show you how to simply visualize the first two principal components of a PCA, by reducing a dataset of 4 dimensions to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c839d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = px.data.iris()\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "fig = px.scatter(components, x=0, y=1, color=df['species'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94854304",
   "metadata": {},
   "source": [
    "## Visualize PCA with `px.scatter_3d`\n",
    "\n",
    "With `px.scatter_3d`, you can visualize an additional dimension, which let you capture even more variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = px.data.iris()\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    components, x=0, y=1, z=2, color=df['species'],\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0acf5",
   "metadata": {},
   "source": [
    "## Plotting explained variance\n",
    "\n",
    "Often, you might be interested in seeing how much variance PCA is able to explain as you increase the number of components, in order to decide how many dimensions to ultimately keep or analyze. This example shows you how to quickly plot the cumulative sum of explained variance for a high-dimensional dataset like [Diabetes](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset).\n",
    "\n",
    "With a higher explained variance, you are able to capture more variability in your dataset, which could potentially lead to better performance when training your model. For a more mathematical explanation, see this [Q&A thread](https://stats.stackexchange.com/questions/22569/pca-and-proportion-of-variance-explained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "boston = load_diabetes()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(df)\n",
    "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "px.area(\n",
    "    x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "    y=exp_var_cumul,\n",
    "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea9197",
   "metadata": {},
   "source": [
    "## Visualize Loadings\n",
    "\n",
    "It is also possible to visualize loadings using `shapes`, and use `annotations` to indicate which feature a certain loading originally belonged to. Here, we define loadings as:\n",
    "\n",
    "$$\n",
    "loadings = eigenvectors \\cdot \\sqrt{eigenvalues}\n",
    "$$\n",
    "\n",
    "For more details about the linear algebra behind eigenvectors and loadings, see this [Q&A thread](https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = px.data.iris()\n",
    "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "X = df[features]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "fig = px.scatter(components, x=0, y=1, color=df['species'])\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    fig.add_annotation(\n",
    "        ax=0, ay=0,\n",
    "        axref=\"x\", ayref=\"y\",\n",
    "        x=loadings[i, 0],\n",
    "        y=loadings[i, 1],\n",
    "        showarrow=True,\n",
    "        arrowsize=2,\n",
    "        arrowhead=2,\n",
    "        xanchor=\"right\",\n",
    "        yanchor=\"top\"\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=loadings[i, 0],\n",
    "        y=loadings[i, 1],\n",
    "        ax=0, ay=0,\n",
    "        xanchor=\"center\",\n",
    "        yanchor=\"bottom\",\n",
    "        text=feature,\n",
    "        yshift=5,\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03096098",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Learn more about `px`, `px.scatter_3d`, and `px.scatter_matrix` here:\n",
    "* https://plot.ly/python/plotly-express/\n",
    "* https://plot.ly/python/3d-scatter-plots/\n",
    "* https://plot.ly/python/splom/\n",
    "\n",
    "The following resources offer an in-depth overview of PCA and explained variance:\n",
    "* https://en.wikipedia.org/wiki/Explained_variation\n",
    "* https://scikit-learn.org/stable/modules/decomposition.html#pca\n",
    "* https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579\n",
    "* https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another\n",
    "* https://stats.stackexchange.com/questions/22569/pca-and-proportion-of-variance-explained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e480a",
   "metadata": {},
   "source": [
    "### What About Dash?\n",
    "\n",
    "[Dash](https://dash.plot.ly/) is an open-source framework for building analytical applications, with no Javascript required, and it is tightly integrated with the Plotly graphing library.\n",
    "\n",
    "Learn about how to install Dash at https://dash.plot.ly/installation.\n",
    "\n",
    "Everywhere in this page that you see `fig.show()`, you can display the same figure in a Dash application by passing it to the `figure` argument of the [`Graph` component](https://dash.plot.ly/dash-core-components/graph) from the built-in `dash_core_components` package like this:\n",
    "\n",
    "```python\n",
    "import plotly.graph_objects as go # or plotly.express as px\n",
    "fig = go.Figure() # or any Plotly Express function e.g. px.bar(...)\n",
    "# fig.add_trace( ... )\n",
    "# fig.update_layout( ... )\n",
    "\n",
    "from dash import Dash, dcc, html\n",
    "\n",
    "app = Dash()\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(figure=fig)\n",
    "])\n",
    "\n",
    "app.run_server(debug=True, use_reloader=False)  # Turn off reloader if inside Jupyter\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "plotly": {
   "description": "Visualize Principle Component Analysis (PCA) of your high-dimensional data in Python with Plotly.",
   "display_as": "ai_ml",
   "language": "python",
   "layout": "base",
   "name": "PCA Visualization",
   "order": 4,
   "page_type": "u-guide",
   "permalink": "python/pca-visualization/",
   "thumbnail": "thumbnail/ml-pca.png"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
